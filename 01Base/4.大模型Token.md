<h1 id="cuQvg">引言</h1>
在人工智能领域，大型语言模型（LLM）如GPT-4、Claude等系统性地改变了人机交互方式。



![](https://cdn.nlark.com/yuque/0/2025/png/45054063/1761042208893-69951013-3664-4561-a80c-ecadbd2d550b.png)



这些模型处理文本的核心单元被称为"Token"，这个看似简单的概念实则蕴含复杂的工程设计和语言学原理。本文将深入解析Token的本质、技术实现及其在模型运作中的关键作用。



推荐视频：[https://www.bilibili.com/video/BV1smXUYSEGi?t=2.4](https://www.bilibili.com/video/BV1smXUYSEGi?t=2.4)

---



<h1 id="tHAxM">Token化技术全景图</h1>
<h2 id="zcW84">核心处理流程</h2>
```latex
原始文本 → 预处理 → 分词算法 → 词表映射 → 模型输入
            ↓          ↓           ↓
        大小写转换   子词拆分策略   特殊Token添加
        标点规范化   合并规则      位置编码
```



<h1 id="C3DX7">Token的本质定义</h1>
<h2 id="bZb1E">语言的基本粒子</h2>
Token是自然语言在计算机中的离散化表示单元，相当于：

+ 人类语言的"原子"结构。
+ 机器理解的"最小语义单位"。
+ 文本与数值的转换桥梁。



<h2 id="tWcpR">三大主流算法对比</h2>
| 概念 | 传统NLP | 大模型Token化 |
| --- | --- | --- |
| 基本单元 | 单词/字 | 子词(subword) |
| 处理方式 | 规则分词 | 统计学习分词 |
| 词表规模 | 数万级别 | 数万-数十万级别 |
| 未登录词处理 | OOV问题严重 | 通过子词组合解决 |




<h2 id="bFOK1">与传统NLP概念的对比</h2>
| 特性 | BPE (Byte Pair Encoding) | WordPiece | SentencePiece |
| --- | --- | --- | --- |
| 合并策略 | 频率优先 | 语言模型概率最大 | 数据驱动优化 |
| 处理单位 | UTF-8字符 | 初始词汇单元 | 原始字节流 |
| 空格处理 | 需特殊标记 | 需特殊标记 | 内置空格处理 |
| 多语言支持 | 需预处理 | 需预处理 | 原生支持 |
| 典型应用 | GPT系列 | BERT家族 | T5、LLaMA |




<h1 id="ytwBv">Token的技术实现原理</h1>
<h2 id="brN42">分词算法演进</h2>
```python
# 传统分词示例（结巴分词）
import jieba
text = "深度学习改变世界"
print(jieba.lcut(text))  # ['深度', '学习', '改变', '世界']

# BPE分词示例（简化版）
def byte_pair_encoding(text, vocab_size):
    # 统计字符频率→合并最高频组合→迭代至目标词表大小
    return subwords
```



<h2 id="kmGgJ">主流分词方法</h2>
<h3 id="IC7mo">BPE（Byte-Pair Encoding）</h3>
+ GPT系列采用。
+ 通过合并高频字符对构建词表。



算法步骤演示（以"自然语言处理"为例）  
原始词频统计：

```latex
自:5, 然:5, 语:5, 言:5, 处:5, 理:5
自然:3, 语言:3, 处理:3
自然语言:1, 自然语言处理:1
```



迭代合并过程：

1. 初始词汇：{自,然,语,言,处,理}
2. 第一轮合并："自"+"然"→"自然"（共现3次）
3. 第二轮合并："语"+"言"→"语言"（共现3次）
4. 第三轮合并："自然"+"语言"→"自然语言"
5. 最终词表包含子词：["自然语言", "处理", "自然", "语言", ...]



代码实现示例：

```python
from tokenizers import Tokenizer, models, pre_tokenizers, trainers

# 初始化BPE模型
bpe_tokenizer = Tokenizer(models.BPE())
bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

# 训练配置
trainer = trainers.BpeTrainer(
    vocab_size=50000,
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
)

# 训练过程
corpus = ["自然语言处理是人工智能的核心领域...", "深度学习推动NLP发展..."]
bpe_tokenizer.train_from_iterator(corpus, trainer)

# 分词演示
output = bpe_tokenizer.encode("自然语言处理模型")
print(output.tokens)  # ["自然", "语言", "处理", "模", "型"]
```



<h3 id="x1QDj">WordPiece</h3>
+ BERT采用。
+ 基于语言模型概率的合并策略。
+ 核心公式(合并选择依据)。



![](https://cdn.nlark.com/yuque/0/2025/png/45054063/1761042456648-46af0e82-b359-442e-b5f6-67ac95bd186d.png)



<h3 id="yY0FX">与BPE的关键差异</h3>


![](https://cdn.nlark.com/yuque/0/2025/png/45054063/1761042496713-113c7066-9e2d-46d7-a709-cae9950411aa.png)



实际应用示例

输入文本：

```latex
"unbelievable performance"
```



处理过程：

1. 初始拆分：`["u", "n", "b", "e", "l", "i", "e", "v", "a", "b", "l", "e", "per", "form", "ance"]`
2. 逐步合并：

```latex
"per"+"form" → "perform" (score=0.82)
"perform"+"ance" → "performance" (score=0.76)
```

3. 最终输出：`["un", "##belie", "##vable", "performance"]`



<h3 id="oy43K">SentencePiece</h3>
+ 支持任意格式文本。
+ 直接处理原始字节流。

```latex
3.1 技术突破点
统一空格处理：将空格转换为▁符号
"Hello world" → ["▁He", "llo", "▁world"]

字节回退机制：未知字符用UTF-8字节表示
"😊" → [0xF0, 0x9F, 0x98, 0x8A]

无损反转：支持Token序列→原始文本的完美还原

3.2 多语言处理实例
混合文本输入：
"This is こんにちは！안녕하세요?"

处理结果
{
  "tokens": [
    "▁This", "▁is", "▁", "こん", "にち", "は", "!", 
    "안", "녕", "하", "세", "요", "?"
  ]
}
```



<h2 id="Fmb2E">词表构建过程</h2>
1. 语料预处理：标准化、去噪、分词。
2. 频率统计：统计字符/子词共现频率。
3. 合并策略：按算法规则迭代合并单元。
4. 词表优化：平衡覆盖率和计算效率。



<h2 id="LSXZx">典型词表参数</h2>
| 模型 | 词表大小 | 特殊Token数量 |
| --- | --- | --- |
| GPT-2 | 50,257 | 20+ |
| BERT | 30,522 | 5 |
| LLaMA | 32,000 | 10 |
| PaLM | 256,000 | 15 |




<h2 id="RgkCv">向量空间映射</h2>
![](https://cdn.nlark.com/yuque/0/2025/png/45054063/1761042606565-b3d85e16-b75a-46cc-8b80-f8a7ed49c004.png)

+ 每个Token对应d维向量（通常d=1024-12288）。
+ Embedding矩阵维度：词表大小 × 隐藏层维度。



<h2 id="tClga">位置编码</h2>
![](https://cdn.nlark.com/yuque/0/2025/png/45054063/1761042624350-5a79366c-5f03-4c1d-9337-218074bfc4de.png)

+ 解决Transformer的位置感知问题。
+ 每个Token携带绝对位置信息。



<h2 id="EoA9E">注意力机制中的交互</h2>
![](https://cdn.nlark.com/yuque/0/2025/png/45054063/1761042644020-fff85ebb-5e74-446e-bf32-527fbe0b4f04.png)

+ Token通过自注意力建立全局关联。
+ 上下文窗口内的动态关系建模。



<h1 id="Q4Y5A">Token的核心作用</h1>
<h2 id="Kn0NZ">信息压缩</h2>
将可变长度文本转换为固定维度张量。

+ 典型压缩比例：
    - 英文：1 Token ≈ 4字符。
    - 中文：1 Token ≈ 2.5汉字。



<h2 id="jVxvr">语义理解</h2>
+ 通过子词组合保留词汇的形态学特征。
    - 例如，“unhappiness”可以被分解为["un", "happy", "ness"]。
+ 提供处理未见词汇的泛化能力。



<h2 id="ODu2m">计算优化</h2>
相比于字符级处理方法：

+ 减少序列长度50-80%。
+ 降低计算复杂度。



相比单词级处理方法：

+ 词表缩小3-5倍。
+ 缓解数据稀疏问题。



<h2 id="D4tU0">多模态扩展</h2>
+ 图像Token：使用ViT（Vision Transformer）将图像分割成16x16像素单元。
+ 音频Token：利用SoundStream进行离散音频编码。
+ 实现跨模态统一表示。



<h1 id="ScvTi">不同模型的Token策略对比</h1>
<h2 id="MLJt7">分词粒度差异</h2>
| 模型类型 | 典型Token长度 | 特点 |
| --- | --- | --- |
| BERT | 中文：字级 | 更好捕捉汉字组合特征 |
| GPT-3 | 子词级 | 平衡效率和语义 |
| T5 | SentencePiece | 支持跨语言统一处理 |




<h2 id="LczcI">长度限制对比</h2>
| 模型 | 最大Token数 | 突破方式 |
| --- | --- | --- |
| GPT-4 | 32k | 窗口滑动+记忆压缩 |
| Claude 2 | 100k | 改进位置编码 |
| MPT | 65k | ALiBi（Attention with Linear Biases）位置偏置 |




<h1 id="hpCj1">实践中的关键问题</h1>
<h2 id="dABij">Token长度优化</h2>
<h4 id="IGY9H">压缩技巧：</h4>
+ 删除停用词（保留信息量>90%）
+ 实体合并（例如将长数字转换为[NUM]）



<h4 id="4b1b213d">扩展方法：</h4>
+ 分块处理+记忆重组
+ 层次化注意力机制



<h2 id="Olu21">多语言适配</h2>
<h4 id="xl0dP">混合语料训练导致的问题：</h4>
+ 中日韩文Token效率较低
+ 右向左语言处理异常



<h4 id="915db64d">解决方案：</h4>
+ 专用子词切分规则
+ 语言标识符引导



<h2 id="WA4e4">成本影响</h2>
<h4 id="mMyak">API调用成本计算：</h4>
+ 计算方式：输入 + 输出Token总数 × 费率



<h4 id="apzlX">典型场景成本对比：</h4>
| 任务类型 | 平均Token消耗 (输入/输出) | 成本比例 |
| --- | --- | --- |
| 摘要生成 | 500/100 | 1:0.2 |
| 代码生成 | 1200/300 | 1:0.25 |
| 长文档问答 | 8000/500 | 16:1 |




<h1 id="z7LGf">未来演进方向</h1>
<h3 id="vWjNI">Token边界检测</h3>
使用Hugging Face可视化工具：

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "Tokenization is crucial!"
encoding = tokenizer(text, return_offsets_mapping=True)

for token, (start, end) in zip(encoding.tokens(), encoding.offset_mapping):
    print(f"{token.ljust(15)} [{start}:{end}]")
```



输出结果：

```latex
Token          [0:5]
ization        [5:12]
 is            [13:15]
 crucial       [16:23]
!              [23:24]
```



<h3 id="bVU8B">词表覆盖率分析</h3>
```python
import matplotlib.pyplot as plt

vocab = tokenizer.get_vocab()
freq = sorted([cnt for cnt in vocab.values()], reverse=True)

plt.plot(freq)
plt.yscale('log')
plt.xlabel('Token Rank')
plt.ylabel('Frequency')
plt.title('Zipf Law Distribution');
```



<h2 id="x7mdn">选择策略矩阵</h2>
| 场景 | 推荐算法 | 理由 |
| --- | --- | --- |
| 通用英文文本 | BPE | 平衡效率与性能，在英文处理中表现良好 |
| 中文混合专业术语 | WordPiece | 更好处理组合词，适用于复杂词汇 |
| 多语言场景 | SentencePiece | 统一处理不同语言，支持跨语言应用 |
| 需要文本复原 | SentencePiece | 支持无损反转，确保原始文本可恢复 |




<h2 id="IgatZ">参数调优建议</h2>
```latex
vocab_size: 
  英文: 30000-50000
  中文: 40000-60000
  多语言: 100000+

特殊Token:
  必须包含: [UNK], [PAD], [CLS], [SEP]
  推荐添加: [NUM], [URL], [EMAIL]

训练数据量:
  最小数据量: 10MB
  理想数据量: 1GB+
```



<h1 id="lW0lO">未来演进方向</h1>
<h2 id="tpJta">动态Token化</h2>
+ 定义：根据上下文动态调整分词粒度，以提高模型在不同场景下的表现。
+ 案例：Google的Dynamic Tokenization，展示了如何通过适应文本的具体内容来优化Token的选择。



<h2 id="kfZoG">语义单元Token</h2>
+ 概念：直接学习和使用概念级别的Token，超越传统的词汇或子词级别划分。
+ 实例：如MIT提出的"语义原子"研究，旨在让模型理解和处理更高级别的语言结构。



<h2 id="qZAq3">无损压缩技术</h2>
+ 方法：采用字节级模型（例如ByT5）进行处理，在不损失性能的前提下，缩小词表大小，提高效率。
+ 目标：实现高效的信息表示，同时保持对各种语言特性的支持。



<h2 id="pVSSL">神经分词器</h2>
+ 策略：利用端到端的学习方法自动发现最优的分词策略，从而提高处理复杂文本的能力。
+ 应用：华为的PanGu-Σ采用了渐进式分词技术，展示了如何通过深度学习自动优化分词过程，以更好地服务于不同的应用场景。

  


